\documentclass[letterpaper,12pt]{article}
\usepackage{tabularx} % extra features for tabular environment
\usepackage{amsmath}  % improve math presentation
\usepackage{graphicx} % takes care of graphic including machinery
\usepackage[margin=0.95in,letterpaper]{geometry} % decreases margins
\usepackage{cite} % takes care of citations
\usepackage[titletoc,title]{appendix} % takes care of appendices
\usepackage{listings} % code representation
\usepackage{pdflscape}
\usepackage{csquotes} % for quoting existing work
\usepackage{color} % defines colors for code listings
\usepackage{comment} % allows for block of comments
\usepackage[final]{hyperref} % adds hyper links inside the generated pdf file

% style code listings
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4
}
\lstset{style=mystyle}

\begin{document}

\title{CM30225 Parallel Computing\\Assessed Coursework Assignment 1}
\author{Adam Jaamour}
\date{19 November, 2018}
\maketitle


% ------------------------------------------ 1 - INTRODUCTION ------------------------------------------ 

\section{Introduction}

The objective of this assignment is to use low-level primitive parallelism constructs on a shared memory architecture and analyse how parallel problems scale on such an architectures using \textit{C} and \textit{pthreads} on Balena, a mid-sized cluster with 2720 cpu cores\cite{balena_notes}.\\

The background is a method called \textit{relaxation technique}, a solution to differential equations, which is achieved by having a square array of values and repeatedly replacing a value with the average of its four neighbours, excepting boundaries values which remain fixed. This process is repeated until all values settle down to within a given precision.


% ------------------------------------------ 2 - IMPLEMENTATION ------------------------------------------ 

\section{Implementation}

\subsection{Parallelism}
\label{sec:implementation_parallelism}

In this program, shared memory parallelism was implemented in C using POSIX threads \cite{posix_threads}. Amongst the multiple approaches to using \textit{pthreads} for parallelism that exist, one favouring efficiency and minimal overhead was chosen. This was achieved by first writing an efficient sequential version of the program, and then scaling it to parallel by using the least \textit{pthread} management possible, such as avoiding the use of barriers and focusing on mutexes to achieve a solution in parallel.\\

The solution uses two global arrays that can be accessed by all threads simultaneously: a square array of doubles holding the values to average, and a second array made up of mutexes only. After the threads' creation, each of them starts looping iteratively through the square array of double values. Whenever a thread accesses a value to read and update it, it locks access to that value as well as to its four neighbours that are used to calculate the new average. The locked values are only unlocked once their updated value has been written.\\

The thread locking a value in the array of doubles uses the mutex positioned in the same location in the second array of mutexes to lock the value being updated (and its four neighbour values). For example, if a thread is accessing the double value located at position $[i=4; j=6]$ in the first array, then it will use the mutex located at the same position in the second array of mutexes to lock the value. See Listing \ref{lst:loop_code} for the implementation in C.\\

\begin{lstlisting}[language=C, caption=Looping through the arrays (main.c), label={lst:loop_code}]
double **square_array;
pthread_mutex_t **mutex_array;

void* relaxation_runner(void* arg) {
    // ...
    for (i = 1; i < dim - 1; i++) {
    	for (j = 1; j < dim - 1; j++) {
    		// lock and retrieve current value to replace
    		pthread_mutex_lock(&mutex_array[i][j]);
    		double old_value = square_array[i][j];
    		// retrieve the 4 surrounding values needed to average
    		double v_left = square_array[i][j-1];
    		double v_right = square_array[i][j+1];
    		double v_up = square_array[i-1][j];
    		double v_down = square_array[i+1][j];
    		// calculate new value, replace the old value and unlock
    		double new_value = (v_left + v_right + v_up + v_down) / 4;
    		square_array[i][j] = new_value;
    		pthread_mutex_unlock(&mutex_array[i][j]);
    		// ...
    	}
    }
}
\end{lstlisting}

This solution can be pictured as a game of snake \cite{snake_game}, with the first thread representing the snake's head and the last thread representing its tail. The first thread iterates through the array starting at $[i=1; j=1]$, updating the value at this location, before moving on to $[i=1; j=2]$. As the first thread is moving through the array, the second thread and all the others will follow the same path as the first thread once they are scheduled. If they catch up with the first thread, they will have wait for it to unlock the value it is updating before accessing it, like a game of snake where the each body part follows the part in front of it once it moves.\\

This might cause a lot of overhead for very small array dimensions, but will cause less overhead as the arrays are larger in size. Indeed, arrays with dimensions of $5000\cdot5000$, equivalent to 25 million values in the array, can be solved in less than 10 minutes on Balena using a precision of $0.01$. See Section \ref{sec:analysis} for more information on the results of this implementation.

\subsection{Code}

\subsubsection{Structure}

The code was written following the C89 standard. It is divided in three source files to organize the code in logical sections:

\begin{itemize}
    \item main.c: The main code where the program starts executing. Contains the threaded function, \textit{relaxation\_runner}, that averages the array by iterating through it. It also contains all the global variables used throughout the program i.e. the program parameters such as array dimensions, precision to average to, number of threads to use and the initial array of double values.
    \item array\_helpers.c: This source file contains functions to allocate space to arrays, initialise them and populate them with values.
    \item print\_helpers.c: This source file hosts functions used to print data to the command line.
\end{itemize}

\subsubsection{Additional notes}

The function \textit{gettimeofday} from the \textit{sys/time.h} library \cite{c_systime} was used to calculate the time needed to find a solution. It started recording time before creating the threads and stopped right after all the threads stopped executing. See Listing \ref{lst:time_code} for the implementation of time.

\begin{lstlisting}[language=C, caption=Recording time (main.c), label={lst:time_code}]
#include <sys/time.h>
struct timeval time1, time2;

int main(int argc, char *argv[]) {
    // start recording time
	gettimeofday(&time1, NULL);
	// create threads
	for (i = 0; i < num_thr; i++) {
		pthread_create(&tids[i], &attr, relaxation_runner, &args[i]);
	}
	// wait until threads finish running
	for (i = 0; i < num_thr; i++) {
		pthread_join(tids[i], NULL);
	}
	// stop recording time
	gettimeofday(&time2, NULL); 
}
\end{lstlisting}

An important point to note is the effect of printing statements on the program's speed. Indeed, printing information on the command line using the \textit{printf} function considerably slows down the program, especially when these are located inside the threaded function's for loops. This means that \textit{printf} could sometimes be called millions of times in a single execution of the program, which distorted the final results. The solution was to use a global boolean as a condition to execute the calls to \textit{printf}, allowing for easy turning on/off print statements. 

\subsection{Balena}

The program needed to be run on Balena hundreds of times using different parameters, namely varying array sizes and number of threads, to gather relevant data. This meant that some automation was required to avoid executing each program one at a time manually.\\

The first automation process was the \textit{makefile}, which allowed for quick compilation of the three source files into \textit{.exe} executable files (See Appendix\ref{sec:makefile}). The dimension of the array would be specified in the \textit{main.c} source file before being compiled.\\

Next, a bash script was used to automate the submissions to Balena. It would submit 16 jobs at a time to Balena using the SLURM script (see Appendix \ref{sec:slurm}), each with a different number of threads passed as a command line argument (see Appendix \ref{sec:bash}). The first submission for a single array dimension would run on 1 to 16 threads (with increments of 1), and the second submission would run on 20 to 95 threads (with increments of 5).\\

Once the jobs finished executing on the cluster, they would be stored in directories named according on the parameters used. For example, jobs that were run to solve arrays dimensions of 1000 to a precision of 0.01 using 16 cores were stored in "dim1000\_\_cores16\_\_prec01". After all the resulting files were produced, they were exported to a plain text file where a regular expression was used to extract the solving durations for each program that ran with different parameters. These values were then pasted into an Excel spreadsheet where they were organised and plotted into graphs for analysis in Section \ref{sec:analysis}.\\

\subsection{Correctness Testing}

To ensure that the program was correctly solving the initial array, small parameters were used i.e. array dimension of 5, precision of 0.1 and array values ranging from 1 to 10. This allowed for writing down the process of solving the initial array on paper, and comparing the final array with the program output. The assumption that the program being correct for small arrays would be correct for larger arrays as well was made. When transitioning from sequential to parallel solving, the same initial array for the paper testing was used to check that the final array was correct after every update carried out on the code. Additionally, any warnings provided by GCC were fixed with the use of multiple flags such as \textit{Wall}, \textit{Wextra} and \textit{Wconversion}, ensuring that the C code itself was correct.


% ------------------------------------------ 3 - ANALYSIS ------------------------------------------ 

\section{Analysis}
\label{sec:analysis}

A total of 448 jobs were submitted to Balena to gather all the data needed to analyse the program (see Appendix \ref{sec:raw_data}). These jobs were run using the following parameters:
\begin{itemize}
    \item Ten different array dimensions, ranging from 5 to 5000, with random double values generated between 0 and 100. The array dimensions are organised in two parts: large arrays for dimensions of 500, 1000, 2500, 4500 and 5000, and small arrays for dimensions of 5, 10, 25, 50 and 100.
    \item Thirty-two different thread configurations, ranging from a single thread to 95.
    \item Two different numbers of core configurations: 16 and 4 cores.
    \item A precision of 0.01.
\end{itemize}

Plotting the raw output results, which represent the time needed to solve the array in seconds, does not provide much insight into the advantages of running the program in parallel rather than sequentially. Figure \ref{fig:time} tells us that larger arrays take more time to solve than smaller arrays, especially when the number of threads is small (approximately under 10). However, these large arrays seem to make the most of an increasing number of threads, allowing them to be solved faster as the number of threads increases, whereas small arrays do not gain speed from more threads. Indeed, looking at the raw data in Appendix \ref{sec:raw_data}, very small arrays of sizes 5, 10 and 25 actually slow down as the number of threads used rises.\\

\begin{figure}[h]
\centerline{\includegraphics[width=\textwidth]{report/plots/time.png}}
\caption{\label{fig:time}Graph depicting the time needed to solve the array in seconds.}
\end{figure}

Solely analysing the time needed to solve the arrays does not provide enough insights to fully evaluate the gains of running a program in parallel. However, the time needed to solve the arrays in parallel can be directly compared with the time needed to solve the same arrays sequentially. This measure, known as the \textit{speedup}, is calculated as follows: $speedup = time_{sequential} / time_{parallel}$. Figure \ref{fig:speedup_big} illustrates how large arrays ranging from dimensions of 500 to 5000 benefit from speedups of 2.5 to 4.5. It is also very important to notice how the speedup increases in a linear fashion when running in parallel with 1 to 16 threads, but that as soon as more than 16 threads are used, the speedup flattens out and eventually starts slowly decreasing.\\

\begin{figure}[h]
\centerline{\includegraphics[width=\textwidth]{report/plots/speedup_big.png}}
\caption{\label{fig:speedup_big}Graph depicting the speedup gained for large-dimension arrays.}
\end{figure}

A hypothesis can be made regarding how Balena affects the speeds of the program running in parallel when more than 16 threads are being used. According the ``Notes on Balena'' available on the course's webpage \cite{balena_notes}, Balena has: 
\begin{displayquote}
``170 “standard” nodes of two Intel Xeon E5-2650v2 IvybridgeV2 CPUs, each having 8 cores, thus 16 symmetric shared memory cores on a node, giving us 2720 CPU cores.''
\end{displayquote} 
Each job submitted on Balena ran on a single node (see Appendix \ref{sec:slurm}, line 6), meaning 16 cores were available to the program every time it ran on the cluster. This can be an explanation as to why the program ran efficiently up until 16 threads with high speedups before starting to decrease in speed as more threads were used. It is therefore safe to assume that the hypothesis is true and that the operating system scheduled the threads efficiently when there were no more than 16. A reason for this could be that the OS scheduled each thread so that each thread fully used a single CPU to solve the array. As more threads than CPUs were created, the overhead of scheduling the threads across the 16 CPUs increased, thus causing the program to slow down considerably. This is also a reason as to why the highest speedup occurs when exactly 16 threads are in use.\\

When solving smaller arrays ranging from dimensions of 5 to 100, as represented in Figure \ref{fig:speedup_small}, the speedup is less significant than for larger arrays. As a matter of fact, the speedup is inferior to 1 for arrays under dimensions of 100, meaning that the sequential version of the program is quicker than the parallel one. This is mainly due to the fact that the small array size causes threads to often wait for each other as values are locked by the thread currently updating them (see Section \ref{sec:implementation_parallelism} for more information on the parallel implementation). Threads trying to access the same values occur less often for larger arrays than for smaller arrays, explaining the reason why the speedup is much higher for larger arrays (See Figure \ref{fig:speedup_big}) than for smaller arrays (See Figure \ref{fig:speedup_small}).\\

\begin{figure}[h]
\centerline{\includegraphics[width=\textwidth]{report/plots/speedup_small.png}}
\caption{\label{fig:speedup_small}Graph depicting the speedup gained for small-dimension arrays.}
\end{figure}

Another more detailed method of analysing the advantages (or disadvantages) of running a program in parallel rather than sequentially is by calculating the \textit{efficiency}. The efficiency can be calculated by using the speedup previously calculated with the number of threads used as follows: $efficiency = time_{sequential} / (p \cdot time_{parallel})$, where $p$ corresponds to the number of threads used.\\

Using the efficiency confirms the observations made from the speedup graphs. Indeed, the efficiency for large arrays resides around the 20\% to 30\% slice, as seen in Figure \ref{fig:efficiency_big}, meaning that approximately less than one third of processors' power is being used for the computation, with the rest being lost in overheads such as communication, printing or idling while waiting for array values to be unlocked. The same cutoff as in Figure \ref{fig:speedup_big} can be seen when \textit{number of threads used = 16}, where the efficiency considerably drops more than 16 threads are used, thus reinforcing the hypothesis made earlier about the 16 cores in a node being only efficiently used when there are 16 threads or less.\\

\begin{figure}[h]
\centerline{\includegraphics[width=\textwidth]{report/plots/efficiency_big.png}}
\caption{\label{fig:efficiency_big}Graph depicting the efficiency of running in parallel for large-dimension arrays.}
\end{figure}

Similarly to the speedup for smaller arrays, the efficiency is less important for small arrays than it is for large arrays as the efficiency quickly converges towards 0.001\% as more threads are used. Figure \ref{fig:efficiency_small} clearly portrays how inefficient parallelism is for small sized-problems as the program is the most efficient when running on a single thread. Running on four threads already makes the program three times less efficient than running on one thread.\\

\begin{figure}[h]
\centerline{\includegraphics[width=\textwidth]{report/plots/efficiency_small.png}}
\caption{\label{fig:efficiency_small}Graph depicting the efficiency of running in parallel for small-dimension arrays.}
\end{figure}

It is important to note that during the data gathering phase, jobs using 4 cores instead of 16 were submitted to Balena. This was done by changing a line in the SLURM script from ``\textit{\#SBATCH --ntasks-per-node=16}'' to ``\textit{\#SBATCH --ntasks-per-node=4}''. The same test parameters were used for arrays of dimensions 100, 1000 and 2500, running on 1 to 95 threads with a precision of 0.01. However, the resulting data was near identical to the data produced when running on 16 cores, probably indicating that the jobs were not submitted correctly to run on 4 cores. Therefore, the results were not provided in this report as they too similar to the ones produced when running on 16 cores and would not provided any additional information.\\


% ------------------------------------------ 4 - CONCLUSION ------------------------------------------ 

\section{Conclusions}
There are many different ways of implementing parallelism in a program in a shared memory architecture. The analysis of the time, speedup and efficiency of running a program in parallel provided useful information regarding the scalability of such programs. Indeed, on the one hand, it is obvious that parallel programs scale well for large problems, whereas on the other hand this kind of solution does not suit small problems. \\

Another important point is that too much parallelism slows down the program considerably. All of the graphs in this report show that as more threads were used, the execution time rose and the speedup and efficiency diminished. Using too many parallel functions or constructs would have slowed down the program even more. For instance, if barriers were used to sync the threads between each other, an important amount of overhead would have been added in idling alone while the threads joined each other. Waiting threads are the main reason why some parallel implementations of program show poor performance.\\

With this particular implementation of parallelism, the number of CPUs available affected the number of threads that could be used efficiently. Another solution to solve this assignment with parallelism could have been to assign each thread specific locations of the array of values to solve, in which case the number of cores would have less affected the execution speed.\\

In conclusion, parallelism in code exists in multiple forms and will work more efficiently in different conditions for different problem sizes, there is no single solution to parallelism.


% ------------------------------------------ BIBLIOGRAPHY ------------------------------------------ 

\newpage
\bibliographystyle{unsrt}
\bibliography{bibliography.bib}


% ------------------------------------------ APPENDIX ------------------------------------------ 

\newpage
\begin{appendices}

\section{Makefile}
\label{sec:makefile}
\begin{lstlisting}[language=make]
CC			= gcc
CFLAGS		= -Wall -Wextra -Wconversion
LDFLAGS		= -pthread
OBJFILES	= main.o array_helpers.o print_helpers.o
TARGET		= relaxation

all: $(TARGET)

$(TARGET): $(OBJFILES)
	$(CC) $(CFLAGS) -o $(TARGET) $(OBJFILES) $(LDFLAGS)
	
clean:
	rm -rf $(OBJFILES) $(TARGET) *~
\end{lstlisting}

\section{SLURM script}
\label{sec:slurm}

\begin{lstlisting}[language=bash]
#!/bin/bash
#SBATCH --account=cm30225
#SBATCH --partition=teaching
#SBATCH --job-name=aj645_cw1
#SBATCH --output=relaxation.%j.out
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=16
#SBATCH --mail-type=END
#SBATCH --mail-user=aj645@bath.ac.uk
./relaxation $1
\end{lstlisting}

\section{Bash Submission Script}
\label{sec:bash}

\begin{lstlisting}[language=bash]
#!/bin/sh
for i in 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
do
    n=`expr $i \* 1 + 1`  # increment 1 by 1, starting from 1
    # OR
	n=`expr $i \* 5 + 20` # increment 5 by 5, starting from 20
    sbatch job.slurm $n A
done

\end{lstlisting}

\newpage
\section{Raw Data}
\label{sec:raw_data}

\input{report/tables/sequential_table.tex}

\begin{landscape}
\input{report/tables/time_table.tex}
\input{report/tables/speedup_table.tex}
\input{report/tables/efficiency_table.tex}
\input{report/tables/four_cores_table.tex}
\end{landscape}

\end{appendices}
\end{document}