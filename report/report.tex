\documentclass[letterpaper,12pt]{article}
\usepackage{tabularx} % extra features for tabular environment
\usepackage{amsmath}  % improve math presentation
\usepackage{graphicx} % takes care of graphic including machinery
\usepackage[margin=0.95in,letterpaper]{geometry} % decreases margins
\usepackage{cite} % takes care of citations
\usepackage[titletoc,title]{appendix} % takes care of appendices
\usepackage{listings} % code representation
\usepackage{pdflscape}
\usepackage{csquotes} % for quoting existing work
\usepackage{color} % defines colors for code listings
\usepackage{comment} % allows for block of comments
\usepackage[final]{hyperref} % adds hyper links inside the generated pdf file

% style code listings
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4
}
\lstset{style=mystyle}

\begin{document}

\title{CM30225 Parallel Computing\\Assessed Coursework Assignment 1}
\author{\vspace{-5ex}}
\date{\vspace{-5ex}}
\maketitle


% ------------------------------------------ 1 - INTRODUCTION ------------------------------------------ 

\section{Introduction}

The objective of this assignment is to use low-level primitive parallelism constructs on a shared memory architecture and analyse how parallel problems scale on such an architectures using \textit{C} and \textit{pthreads} on Balena, a mid-sized cluster with 2720 cpu cores\cite{balena_notes}.\\

The background is a method called \textit{relaxation technique}, a solution to differential equations, which is achieved by having a square array of values and repeatedly replacing a value with the average of its four neighbours, excepting boundaries values which remain fixed. This process is repeated until all values settle down to within a given precision.


% ------------------------------------------ 2 - IMPLEMENTATION ------------------------------------------ 

\section{Implementation}

\subsection{Parallelism}
\label{sec:implementation_parallelism}

In this program, shared memory parallelism was implemented in C using POSIX threads \cite{posix_threads}. Amongst the multiple approaches to using \textit{pthreads} for parallelism that exist, one favouring efficiency and minimal overhead was chosen. This was achieved by first writing an efficient sequential version of the program, and then scaling it to parallel by using the least \textit{pthread} management possible, such as avoiding the use of barriers and focusing on mutexes to achieve a parallel solution.\\

The solution uses two global arrays that can be accessed by all threads simultaneously: a square array of doubles holding the values to average, and a second array made up of mutexes only. After the threads' creation, each of them starts looping iteratively through the square array of double values. Whenever a thread accesses a value to read and update it, it locks access to that value as well as to its four neighbours that are used to calculate the new average. The locked values are only unlocked once their updated value has been written.\\

The thread locking a value in the array of doubles uses the mutex positioned in the same location in the second array of mutexes to lock the value being updated and its four neighbours. For example, if a thread is accessing the double value located at position $[i=4; j=6]$ in the first array, then it will use the mutex located at the same position in the second array of mutexes to lock the value. See Listing \ref{lst:loop_code} for the implementation in C.\\

\begin{lstlisting}[language=C, caption=Looping through the arrays (main.c), label={lst:loop_code}]
float **square_array;
pthread_mutex_t **mutex_array;

void* relaxation_runner(void* arg) {
    // ...
    for (i = 1; i < dim - 1; i++) {
    	for (j = 1; j < dim - 1; j++) {
    		// lock and retrieve current value to replace
    		pthread_mutex_lock(&mutex_array[i][j]);
    		float old_value = square_array[i][j];
    		// retrieve the 4 surrounding values needed to average
    		float v_left = square_array[i][j-1];
    		float v_right = square_array[i][j+1];
    		float v_up = square_array[i-1][j];
    		float v_down = square_array[i+1][j];
    		// calculate new value, replace the old value and unlock
    		float new_value = (v_left + v_right + v_up + v_down) / 4;
    		square_array[i][j] = new_value;
    		pthread_mutex_unlock(&mutex_array[i][j]);
    		// ...
    	}
    }
}
\end{lstlisting}

This solution can be pictured as a game of snake \cite{snake_game}, with the first thread representing the snake's head and the last thread representing its tail. The first thread iterates through the array starting at $[i=1; j=1]$, updating the value at this location, before moving on to $[i=1; j=2]$. As the first thread is moving through the array, the second thread and all the others will follow the same path as the first thread once they are scheduled. If they catch up with the first thread, they will have wait for it to unlock the value it is updating before accessing it, like a game of snake where the each body part follows the part in front of it once it moves.\\

This ensures that arrays of any dimension can be solved efficiently at high speeds, both sequentially and in parallel. Indeed, arrays with dimensions of $5000\cdot5000$, equivalent to 25 million values in the array, can be solved in less than 10 minutes on Balena using a precision of $0.01$. See Section \ref{sec:analysis} for more information on the results of this implementation.

\subsection{Code}

\subsubsection{Structure}

The code was written following the C89 standard. It is divided in three source files to organize the code in logical sections:

\begin{itemize}
    \item main.c: The main code where the program starts executing. Contains the threaded function, \textit{relaxation\_runner}, that averages the array by iterating through it. It also contains all the global variables used throughout the program i.e. the program parameters such as array dimensions, precision to average to, number of threads to use and the initial array of double values.
    \item array\_helpers.c: This source file contains functions to allocate space to arrays, initialise them and populate them with values.
    \item print\_helpers.c: This source file hosts functions used to print data to the command line.
\end{itemize}

\subsubsection{Additional notes}

The function \textit{gettimeofday} from the \textit{sys/time.h} library \cite{c_systime} was used to calculate the time needed to find a solution. It started recording time before creating the threads and stopped right after all the threads stopped executing. See Listing \ref{lst:time_code} for the implementation of time.

\begin{lstlisting}[language=C, caption=Recording time (main.c), label={lst:time_code}]
#include <sys/time.h>
struct timeval time1, time2;

int main(int argc, char *argv[]) {
    // start recording time
	gettimeofday(&time1, NULL);
	// create threads
	for (i = 0; i < num_thr; i++) {
		pthread_create(&tids[i], &attr, relaxation_runner, &args[i]);
	}
	// wait until threads finish running
	for (i = 0; i < num_thr; i++) {
		pthread_join(tids[i], NULL);
	}
	// stop recording time
	gettimeofday(&time2, NULL); 
}
\end{lstlisting}

An important point to note is the effect of printing statements on the program's speed. Indeed, printing information on the command line using the \textit{printf} function considerably slows down the program, especially when these are located inside the threaded function's for loops. This means that \textit{printf} could sometimes be called millions of times in a single execution of the program, which distorted the final results. The solution was to use a global boolean as a condition to execute the calls to \textit{printf}, allowing for easy turning on/off print statements. 

\subsection{Balena}

The program needed to be run on Balena hundreds of times using different parameters, namely varying array sizes and number of threads, to gather relevant data. This meant that some automation was required to avoid executing each program one at a time manually.\\

The first automation process was the \textit{makefile}, which allowed for quick compilation of the three source files into \textit{.exe} executable files (See Appendix\ref{sec:makefile}). The dimension of the array would be specified in the \textit{main.c} source file before being compiled.\\

Next, a bash script was used to automate the submissions to Balena. It would submit 16 jobs at a time to Balena using the SLURM script (see Appendix \ref{sec:slurm}), each with a different number of threads passed as a command line argument (see Appendix \ref{sec:bash}). The first submission for a single array dimension would run on 1 to 16 threads (with increments of 1), and the second submission would run on 20 to 95 threads (with increments of 5).\\

Once the jobs finished executing on the cluster, they would be stored in directories named according on the parameters used. For example, jobs that were run to solve arrays dimensions of 1000 to a precision of 0.01 using 16 cores were stored in "dim1000\_\_cores16\_\_prec01". After all the resulting files were produced, they were exported to a plain text file where a regular expression was used to extract the solving durations for each program that ran with different parameters. These values were then pasted into an Excel spreadsheet where they were organised and plotted into graphs for analysis in Section \ref{sec:analysis}.\\

\subsection{Correctness Testing}

To ensure that the program was correctly solving the initial array, small parameters were used i.e. array dimension of 5, precision of 0.1 and array values ranging from 1 to 10. This allowed for writing down the process of solving the initial array on paper, and comparing the final array with the program output. The assumption that the program being correct for small arrays would be correct for larger arrays as well was made. When transitioning from sequential to parallel solving, the same initial array for the paper testing was used to check that the final array was correct after every update carried out on the code. Additionally, any warnings provided by GCC were fixed with the use of multiple flags such as \textit{Wall}, \textit{Wextra} and \textit{Wconversion}, ensuring that the C code itself was correct.


% ------------------------------------------ 3 - ANALYSIS ------------------------------------------ 

\section{Analysis}
\label{sec:analysis}

A total of 448 jobs were submitted to Balena to gather all the data needed to analyse the program (see Appendix \ref{sec:raw_data}). These jobs were run using the following parameters:
\begin{itemize}
    \item Ten different array dimensions, ranging from 5 to 5000, with random values generated between 0 and 100. The array dimensions are organised in two parts: large arrays for dimensions of 500, 1000, 2500, 4500 and 5000, and small arrays for dimensions of 5, 10, 25, 50 and 100.
    \item Thirty-two different thread configurations, ranging from a single thread to 95.
    \item Two different numbers of core configurations: 16 and 4 cores.
    \item A precision of 0.01.
\end{itemize}

Plotting the raw output results, which represent the time needed to solve the array in seconds, does not provide much insight into the advantages of running the program in parallel rather than sequentially. Figure \ref{fig:time} tells us that larger arrays take more time to solve than smaller arrays, especially when the number of threads is small (approximately under 10). However, they seem to make the most of a an increasing number of threads, allowing them to be solved faster, whereas small arrays do not gain speed from more threads. Indeed, looking at the raw data in Appendix \ref{sec:raw_data}, very small arrays of sizes 5, 10 and 25 actually slow down as the number of threads used raises.\\

\begin{figure}[h]
\centerline{\includegraphics[width=\textwidth]{report/plots/time.png}}
\caption{\label{fig:time}Graph depicting the time needed to solve the array in seconds.}
\end{figure}

Solely analysing the time needed to solve the arrays does not provide enough insights to fully evaluate the gains of running a program in parallel. However, the time needed to solve the arrays in parallel can be directly compared with the time needed to solve the same arrays sequentially. This measure, known as the \textit{speedup}, is calculated as follows: $time_{sequential} / time_{parallel}$.\\

The speedup clearly shows how parallelism affects the program. Figure \ref{fig:speedup_big} illustrates how large arrays ranging from dimensions of 500 to 5000 benefit from speedups of 2.5 to 4.5. Nonetheless, this isn't the most interesting bit of information. The speedup increases in a linear fashion when running from 1 to 16 threads in parallel, but as soon as more than 16 threads are used, the speedup flattens out and eventually starts decreasing.\\

\begin{figure}[h]
\centerline{\includegraphics[width=\textwidth]{report/plots/speedup_big.png}}
\caption{\label{fig:speedup_big}Graph depicting the speedup gained for large-dimension arrays.}
\end{figure}

This decrease that appears after using more than 16 threads is due to the configuration of Balena. According the ``Notes on Balena'' available on the course's webpage \cite{balena_notes}, Balena has: 
\begin{displayquote}
``170 “standard” nodes of two Intel Xeon E5-2650v2 IvybridgeV2 CPUs, each having 8 cores, thus 16 symmetric shared memory cores on a node, giving us 2720 CPU cores.''
\end{displayquote} 
Each job submitted on Balena ran on a single node (see Appendix \ref{sec:slurm}), meaning 16 cores were available to the program every time it ran on the cluster. This explains why the program ran efficiently up until 16 threads with high speedups before starting to decrease in speed, and consequently in speedup, as more threads were used. It is safe to assume that the operating system scheduled the threads such that each one of them fully used a single CPU while the program was running. As more threads than CPUs were created, the overhead of scheduling the threads across the 16 CPUs increased, thus causing the program to slow down considerably.\\

When solving smaller arrays ranging from dimensions of 5 to 100, as represented in Figure \ref{fig:speedup_small}, the speedup is less significant than for larger arrays. As a matter of fact, the speedup is inferior to 1 for arrays under dimensions of 100, meaning that the sequential version of the program is quicker than the parallel one. This is mainly due to the fact that the small array size causes threads to often wait for each other as values are locked by the thread currently updating them (see Section \ref{sec:implementation_parallelism} for more information on the parallel implementation). Threads trying to access the same values occur less often for larger arrays tahn for smaller arrays, explaining the reason why the speedup is much higher for larger arrays (See Figure \ref{fig:speedup_big}) than for smaller arrays (See Figure \ref{fig:speedup_small}).\\

\begin{figure}[h]
\centerline{\includegraphics[width=\textwidth]{report/plots/speedup_small.png}}
\caption{\label{fig:speedup_small}Graph depicting the speedup gained for small-dimension arrays.}
\end{figure}

Another more advanced method of analysing the advantages (or disadvantages) of running a program in parallel rather than sequentially is by calculating the \textit{efficiency}. The efficiency can be calculated by using the speedup previously calculated with the number of threads used as follows: $time_{sequential} / (p \cdot time_{parallel})$, where $p$ corresponds to the number of threads used.\\

Using the efficiency confirms the observations from the speedup. Indeed, the efficiency for large arrays resides around the 20\% to 30\% slice, as seen in Figure \ref{fig:efficiency_big}. The same cutoff as in Figure \ref{fig:speedup_big} can be seen when \textit{number of threads used = 16}, where the efficiency considerably drops for numbers of threads larger than 16, thus reinforcing the assumption made earlier about the 16 cores in a node being only efficiently used when there are 16 threads or less.

\begin{figure}[h]
\centerline{\includegraphics[width=\textwidth]{report/plots/efficiency_big.png}}
\caption{\label{fig:efficiency_big}Graph depicting the efficiency of running in parallel for large-dimension arrays.}
\end{figure}

Similarly to the speedup for smaller arrays, the efficiency is less important than for larger arrays as it quickly converges towards 0.0\% as more threads are used. Figure \ref{fig:efficiency_small} clearly portrays how inefficient parallelism is for small sized-problems as the program is most efficient when running on a single thread only. Running on 4 threads already makes the program three times less efficient than running on one thread.\\

\begin{figure}[h]
\centerline{\includegraphics[width=\textwidth]{report/plots/efficiency_small.png}}
\caption{\label{fig:efficiency_small}Graph depicting the efficiency of running in parallel for small-dimension arrays.}
\end{figure}

The analysis of the time, speedup and efficiency of running a program in parallel provided useful information regarding the scalability. Indeed, on the one hand, it is obvious that parallel programs scale well for large problems, whereas on the other hand this kind of solution does not suit small problems.\\

It is important to note that during the data gathering phase attempts were made to run the program on Balena using 4 cores instead of 16. This was done by changing a line in the SLURM script to \textit{\#SBATCH --ntasks-per-node=4}. The same test parameters were used for arrays of dimensions 100, 1000 and 2500. However, the resulting data was near identical to the data produced when running on 16 cores. 


% ------------------------------------------ 4 - CONCLUSION ------------------------------------------ 

\section{Conclusions}
There are many different ways of implementing parallelism 
waiting is killing parallelism
many possible solutions, favoured one for large arrays due to little collisions between the threads


% ------------------------------------------ BIBLIOGRAPHY ------------------------------------------ 

\newpage
\bibliographystyle{unsrt}
\bibliography{bibliography.bib}


% ------------------------------------------ APPENDIX ------------------------------------------ 

\newpage
\begin{appendices}

\section{Makefile}
\label{sec:makefile}
\begin{lstlisting}[language=make]
CC			= gcc
CFLAGS		= -Wall -Wextra -Wconversion
LDFLAGS		= -pthread
OBJFILES	= main.o array_helpers.o print_helpers.o
TARGET		= relaxation

all: $(TARGET)

$(TARGET): $(OBJFILES)
	$(CC) $(CFLAGS) -o $(TARGET) $(OBJFILES) $(LDFLAGS)
	
clean:
	rm -rf $(OBJFILES) $(TARGET) *~
\end{lstlisting}

\section{SLURM script}
\label{sec:slurm}

\begin{lstlisting}[language=bash]
#!/bin/bash
#SBATCH --account=cm30225
#SBATCH --partition=teaching
#SBATCH --job-name=aj645_cw1
#SBATCH --output=relaxation.%j.out
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=16
#SBATCH --mail-type=END
#SBATCH --mail-user=aj645@bath.ac.uk
./relaxation $1
\end{lstlisting}

\section{Bash Submission Script}
\label{sec:bash}

\begin{lstlisting}[language=bash]
#!/bin/sh
for i in 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
do
    n=`expr $i \* 1 + 1`  # increment 1 by 1, starting from 1
    # OR
	n=`expr $i \* 5 + 20` # increment 5 by 5, starting from 20
    sbatch job.slurm $n A
done

\end{lstlisting}

\newpage
\section{Raw Data}
\label{sec:raw_data}

\input{report/tables/sequential_table.tex}

\begin{landscape}
\input{report/tables/time_table.tex}
\input{report/tables/speedup_table.tex}
\input{report/tables/efficiency_table.tex}
\input{report/tables/four_cores_table.tex}
\end{landscape}

\end{appendices}
\end{document}